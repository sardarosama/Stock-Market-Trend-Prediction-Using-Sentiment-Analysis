{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf8b628-d5e9-45d8-88cc-9eac40d49878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:158: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "      <td>kickers on my watchlist xide tit soq pnk cpw b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>aap movie 55 percent return for the fea and ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>id be afraid to short amzn to they are looking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>mnta over 12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>oi over 21.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>Industry body CII said #discoms are likely to ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>industry body cii said are likely to suffer a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>#Gold prices slip below Rs 46,000 as #investor...</td>\n",
       "      <td>-1</td>\n",
       "      <td>prices slip below rs 46000 as book profits ami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>Workers at Bajaj Auto have agreed to a 10% wag...</td>\n",
       "      <td>1</td>\n",
       "      <td>workers at bajaj auto have agreed to a 10 perc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>#Sharemarket LIVE: Sensex off day’s high, up 6...</td>\n",
       "      <td>1</td>\n",
       "      <td>live sensex off day’s high up 600 points tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>#Sensex, #Nifty climb off day's highs, still u...</td>\n",
       "      <td>1</td>\n",
       "      <td>climb off days highs still up 2 percent key fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment  \\\n",
       "0     Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1   \n",
       "1     user: AAP MOVIE. 55% return for the FEA/GEED i...          1   \n",
       "2     user I'd be afraid to short AMZN - they are lo...          1   \n",
       "3                                     MNTA Over 12.00            1   \n",
       "4                                      OI  Over 21.37            1   \n",
       "...                                                 ...        ...   \n",
       "5786  Industry body CII said #discoms are likely to ...         -1   \n",
       "5787  #Gold prices slip below Rs 46,000 as #investor...         -1   \n",
       "5788  Workers at Bajaj Auto have agreed to a 10% wag...          1   \n",
       "5789  #Sharemarket LIVE: Sensex off day’s high, up 6...          1   \n",
       "5790  #Sensex, #Nifty climb off day's highs, still u...          1   \n",
       "\n",
       "                                           Text_Cleaned  \n",
       "0     kickers on my watchlist xide tit soq pnk cpw b...  \n",
       "1     aap movie 55 percent return for the fea and ge...  \n",
       "2     id be afraid to short amzn to they are looking...  \n",
       "3                                       mnta over 12.00  \n",
       "4                                         oi over 21.37  \n",
       "...                                                 ...  \n",
       "5786  industry body cii said are likely to suffer a ...  \n",
       "5787  prices slip below rs 46000 as book profits ami...  \n",
       "5788  workers at bajaj auto have agreed to a 10 perc...  \n",
       "5789  live sensex off day’s high up 600 points tests...  \n",
       "5790  climb off days highs still up 2 percent key fa...  \n",
       "\n",
       "[5791 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d1d94c993640caae4552561eb98ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0da52928680424a9bde3b22f1b81bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725a3fc5c3d64dd78efe893b6fe0a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  53\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dell\\Downloads\\Twitter_Stock_Prediction-main\\Bert_Training.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMax length: \u001b[39m\u001b[39m'\u001b[39m, MAX_LEN)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Encode the train and test data for Bert\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m X_train_inputs, X_train_masks \u001b[39m=\u001b[39m preprocessing_for_bert(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m X_test_inputs, X_test_masks \u001b[39m=\u001b[39m preprocessing_for_bert(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# Get the train and test labels\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Dell\\Downloads\\Twitter_Stock_Prediction-main\\Bert_Training.ipynb Cell 1\u001b[0m in \u001b[0;36mpreprocessing_for_bert\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# For each tweet\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m# encode the data. Return input encoding and attention mask\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     encoding \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m             text\u001b[39m=\u001b[39;49mline, \u001b[39m# data to process\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m             add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# adds special chars [CLS] and [SEP] to encoding \u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m             padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m# pad the tweets with 0s to fit max length\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m             max_length \u001b[39m=\u001b[39;49m MAX_LEN, \u001b[39m# assign max length\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m             truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# truncate tweets longer than max length\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m             return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# return tensor as pytorch tensor\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39m# return the attention mask\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# add the encodings to the list\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dell/Downloads/Twitter_Stock_Prediction-main/Bert_Training.ipynb#W0sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoding\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2727\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2717\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2718\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2719\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2720\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2724\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2725\u001b[0m )\n\u001b[1;32m-> 2727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2728\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2729\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2730\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2731\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2732\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2733\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2734\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2735\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2736\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2737\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2738\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2739\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2740\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2741\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2742\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2743\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2744\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2745\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2746\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\transformers\\tokenization_utils.py:652\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[0;32m    653\u001b[0m     first_ids,\n\u001b[0;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39;49msecond_ids,\n\u001b[0;32m    655\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    656\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    657\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    658\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    659\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    660\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    661\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    662\u001b[0m     prepend_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    663\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    664\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    665\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    666\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    667\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    669\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3217\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[1;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   3214\u001b[0m \u001b[39mif\u001b[39;00m return_length:\n\u001b[0;32m   3215\u001b[0m     encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(encoded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 3217\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(\n\u001b[0;32m   3218\u001b[0m     encoded_inputs, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis\n\u001b[0;32m   3219\u001b[0m )\n\u001b[0;32m   3221\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\transformers\\tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    207\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> 211\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\transformers\\tokenization_utils_base.py:695\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39melif\u001b[39;00m tensor_type \u001b[39m==\u001b[39m TensorType\u001b[39m.\u001b[39mPYTORCH:\n\u001b[0;32m    694\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 695\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    696\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     as_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import preprocess\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Read in labeled stock tweet sentiment data\n",
    "data = pd.read_csv('train/stock_data.csv')\n",
    "\n",
    "# Process the Tweets for NLP\n",
    "data = preprocess.Preprocess_Tweets(data)\n",
    "display(data)\n",
    "\n",
    "# Split the training and test data into 80/20 split\n",
    "train_pct = .8\n",
    "np.random.seed(1)\n",
    "idx = np.random.permutation(len(data))\n",
    "\n",
    "X_train = data['Text_Cleaned'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "X_test = data['Text_Cleaned'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "\n",
    "# Prepare the Bert NLP model tokenizer to encode tweets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "# Use this to determine max length for encoding\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Cleaned'].values]\n",
    "MAX_LEN = max([len(sent) for sent in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "\n",
    "\n",
    "# Encode the train and test data for Bert\n",
    "X_train_inputs, X_train_masks = preprocessing_for_bert(X_train)\n",
    "X_test_inputs, X_test_masks = preprocessing_for_bert(X_test)\n",
    "\n",
    "# Get the train and test labels\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "print(X_train_inputs.shape, X_train_masks.shape, y_train_labels.shape)\n",
    "print(X_test_inputs.shape, X_test_masks.shape, y_test_labels.shape)\n",
    "\n",
    "# Set batch size to 16. recommended 16 or 32 depending on GPU size \n",
    "batch_size = 16\n",
    "\n",
    "# Randomize the train data and define dataloader for model training \n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Randomize the test data and define dataloader for model testing\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6e3eca-556a-4ee7-b48e-ea5b9f1bc998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.54333  |  Test Loss: 0.44172  |  Test Accuracy: 77.89\n",
      "Epoch: 2  |  Train Loss: 0.30591  |  Test Loss: 0.43419  |  Test Accuracy: 83.11\n",
      "Epoch: 3  |  Train Loss: 0.14389  |  Test Loss: 0.67964  |  Test Accuracy: 82.62\n",
      "Epoch: 4  |  Train Loss: 0.06126  |  Test Loss: 0.82994  |  Test Accuracy: 83.19\n"
     ]
    }
   ],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Define the neurons for the final layer\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        h_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# Set random seed for repeatability\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Initialize Bert Classifier\n",
    "model = BertClassifier(freeze=False)\n",
    "\n",
    "# Send model to device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "# Define scheduler for training the optimizer \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "\n",
    "# Define cross entropy loss function \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        # Get batch inputs, masks and labels \n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        \n",
    "        # Add loss to the running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update the model weights based on the loss \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch in test_dataloader:\n",
    "        # Get encoding inputs, masks and labels\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Predict the input values without updating the model \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data \n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print epoch information \n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dcfe2b-967c-4969-a086-fb326815bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aal - completed\n",
      "aapl - completed\n",
      "adbe - completed\n",
      "adp - completed\n",
      "adsk - completed\n",
      "akam - completed\n",
      "alxn - completed\n",
      "amat - completed\n",
      "amgn - completed\n",
      "amzn - completed\n",
      "atvi - completed\n",
      "avgo - completed\n",
      "bbby - completed\n",
      "bidu - completed\n",
      "bmrn - completed\n",
      "ca - completed\n",
      "celg - completed\n",
      "cern - completed\n",
      "chkp - completed\n",
      "chtr - completed\n",
      "cmcsa - completed\n",
      "cost - completed\n",
      "csco - completed\n",
      "csx - completed\n",
      "ctrp - completed\n",
      "ctsh - completed\n",
      "disca - completed\n",
      "disck - completed\n",
      "dish - completed\n",
      "dltr - completed\n",
      "ea - completed\n",
      "ebay - completed\n",
      "endp - completed\n",
      "esrx - completed\n",
      "expe - completed\n",
      "fast - completed\n",
      "fb - completed\n",
      "fisv - completed\n",
      "foxa - completed\n",
      "fox - completed\n",
      "gild - completed\n",
      "googl - completed\n",
      "goog - completed\n",
      "hsic - completed\n",
      "ilmn - completed\n",
      "inct - completed\n",
      "incy - completed\n",
      "intu - completed\n",
      "isrg - completed\n",
      "jd - completed\n",
      "khc - completed\n",
      "lbtya - completed\n",
      "lbtyk - completed\n",
      "lltc - completed\n",
      "lmca - completed\n",
      "lmck - completed\n",
      "lrcx - completed\n",
      "lrcx - completed\n",
      "lvnta - completed\n",
      "mar - completed\n",
      "mat - completed\n",
      "mdlz - completed\n",
      "mnst - completed\n",
      "msft - completed\n",
      "mu - completed\n",
      "mxim - completed\n",
      "myl - completed\n",
      "nclh - completed\n",
      "nflx - completed\n",
      "ntap - completed\n",
      "ntes - completed\n",
      "nvda - completed\n",
      "nxpi - completed\n",
      "orly - completed\n",
      "payx - completed\n",
      "pcar - completed\n",
      "pcln - completed\n",
      "pypl - completed\n",
      "qcom - completed\n",
      "qvca - completed\n",
      "regn - completed\n",
      "rost - completed\n",
      "sbac - completed\n",
      "sbux - completed\n",
      "sndk - completed\n",
      "srcl - completed\n",
      "stx - completed\n",
      "swks - completed\n",
      "symc - completed\n",
      "tmus - completed\n",
      "trip - completed\n",
      "tsco - completed\n",
      "tsla - completed\n",
      "txn - completed\n",
      "ulta - completed\n",
      "viab - completed\n",
      "vod - completed\n",
      "vrsk - completed\n",
      "vrtx - completed\n",
      "wba - completed\n",
      "wdc - completed\n",
      "wfm - completed\n",
      "xlnx - completed\n",
      "yhoo - completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Get the list of stock data to convert\n",
    "files = os.listdir('data/')\n",
    "\n",
    "# for each stock files\n",
    "for x in range(len(files)):\n",
    "    # open the excel file on the Stream sheet\n",
    "    stock = pd.read_excel('data/'+files[x] + '/export_dashboard_' + files[x], sheet_name='Stream')\n",
    "\n",
    "    # Assign the ticker name as a column\n",
    "    stock['Ticker'] = files[x].split('_')[0]\n",
    "    \n",
    "    # Convert string date times to datetime\n",
    "    stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "    stock['Hour'] = stock['Hour'].apply(lambda t: pd.Timedelta(hours=int(t[:2]), minutes=int(t[3:])))\n",
    "    stock['Datetime'] = stock['Date'] + stock['Hour']\n",
    "\n",
    "    # Rename column that holds the tweets content\n",
    "    stock.rename(columns = {'Tweet content':'Text'}, inplace = True)\n",
    "\n",
    "    # Pre process the tweet content\n",
    "    stock = preprocess.Preprocess_Tweets(stock)\n",
    "\n",
    "    # Remove excess columns\n",
    "    stock = stock[['Tweet Id', 'Ticker', 'Datetime', 'Text', 'Text_Cleaned', 'Favs', 'RTs', 'Followers', 'Following', 'Is a RT']]\n",
    "    \n",
    "    # Fill NAs in Favs, RTs, Followers and Following with 0\n",
    "    stock = stock.fillna(0)\n",
    "\n",
    "    # Encode processed tweets for Bert NLP model\n",
    "    stock_inputs, stock_masks = preprocessing_for_bert(stock['Text_Cleaned'].values)\n",
    "\n",
    "    # Put stock data in PyTorch dataloader for processing \n",
    "    stock_data = TensorDataset(stock_inputs, stock_masks)\n",
    "    stock_sampler = RandomSampler(stock_data)\n",
    "    stock_dataloader = DataLoader(stock_data, sampler=stock_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Assign model to evaluate \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # For each batch\n",
    "    for batch in stock_dataloader:\n",
    "        # Get encoded inputs and masks \n",
    "        batch_inputs, batch_masks = batch\n",
    "\n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "\n",
    "        # Predict classes with Bert for given inputs \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Convert predictions to 0s and 1s\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predictions.append(preds)\n",
    "\n",
    "    # Combine all batch predictions\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    \n",
    "    # Add predictions to stock dataframe\n",
    "    stock['Sentiment'] = predictions\n",
    "    \n",
    "    # save predictions as new csv\n",
    "    stock.to_csv('data/'+files[x] +'/stock_data_sentiment.csv', index=False)\n",
    "    \n",
    "    # Show stock names as they are completed \n",
    "    print(files[x].split('_')[0], '- completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea57530-2bac-4033-9f83-dcdc3207dd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755011135857461\n",
      "0.8802816901408451\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_inputs, batch_masks, batch_labels = batch\n",
    "\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "        \n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "negatives = np.where(y_test==0)[0]\n",
    "TNs = np.where( (y_test==0) & (y_test==predictions) )[0]\n",
    "print(len(TNs)/len(negatives))\n",
    "\n",
    "positives = np.where(y_test==1)[0]\n",
    "TPs = np.where( (y_test==1) & (y_test==predictions) )[0]\n",
    "print(len(TPs)/len(positives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
