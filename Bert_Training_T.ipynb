{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d215a44",
   "metadata": {},
   "source": [
    "Bert Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93853a65",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7371c20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from torch) (2.8.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dell\\anaconda3\\envs\\streamlit\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#pip install torch\n",
    "#streamlit python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import preprocess\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4330584",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1762cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in labeled stock tweet sentiment data\n",
    "data = pd.read_csv('train/stock_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43ea5c90",
   "metadata": {},
   "source": [
    "Process tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a16d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\streamlit\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:158: FutureWarning: Possible nested set at position 1\n",
      "  pat = re.compile(pat, flags=flags)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "      <td>kickers on my watchlist xide tit soq pnk cpw b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>aap movie 55 percent return for the fea and ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>id be afraid to short amzn to they are looking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>mnta over 12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>oi over 21.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>Industry body CII said #discoms are likely to ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>industry body cii said are likely to suffer a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>#Gold prices slip below Rs 46,000 as #investor...</td>\n",
       "      <td>-1</td>\n",
       "      <td>prices slip below rs 46000 as book profits ami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>Workers at Bajaj Auto have agreed to a 10% wag...</td>\n",
       "      <td>1</td>\n",
       "      <td>workers at bajaj auto have agreed to a 10 perc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>#Sharemarket LIVE: Sensex off day’s high, up 6...</td>\n",
       "      <td>1</td>\n",
       "      <td>live sensex off day’s high up 600 points tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5790</th>\n",
       "      <td>#Sensex, #Nifty climb off day's highs, still u...</td>\n",
       "      <td>1</td>\n",
       "      <td>climb off days highs still up 2 percent key fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Sentiment  \\\n",
       "0     Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1   \n",
       "1     user: AAP MOVIE. 55% return for the FEA/GEED i...          1   \n",
       "2     user I'd be afraid to short AMZN - they are lo...          1   \n",
       "3                                     MNTA Over 12.00            1   \n",
       "4                                      OI  Over 21.37            1   \n",
       "...                                                 ...        ...   \n",
       "5786  Industry body CII said #discoms are likely to ...         -1   \n",
       "5787  #Gold prices slip below Rs 46,000 as #investor...         -1   \n",
       "5788  Workers at Bajaj Auto have agreed to a 10% wag...          1   \n",
       "5789  #Sharemarket LIVE: Sensex off day’s high, up 6...          1   \n",
       "5790  #Sensex, #Nifty climb off day's highs, still u...          1   \n",
       "\n",
       "                                           Text_Cleaned  \n",
       "0     kickers on my watchlist xide tit soq pnk cpw b...  \n",
       "1     aap movie 55 percent return for the fea and ge...  \n",
       "2     id be afraid to short amzn to they are looking...  \n",
       "3                                       mnta over 12.00  \n",
       "4                                         oi over 21.37  \n",
       "...                                                 ...  \n",
       "5786  industry body cii said are likely to suffer a ...  \n",
       "5787  prices slip below rs 46000 as book profits ami...  \n",
       "5788  workers at bajaj auto have agreed to a 10 perc...  \n",
       "5789  live sensex off day’s high up 600 points tests...  \n",
       "5790  climb off days highs still up 2 percent key fa...  \n",
       "\n",
       "[5791 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the Tweets for NLP\n",
    "data = preprocess.Preprocess_Tweets(data)\n",
    "display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74ddf228",
   "metadata": {},
   "source": [
    "Split the data in Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa8ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and test data into 80/20 split\n",
    "train_pct = .8\n",
    "np.random.seed(1)\n",
    "idx = np.random.permutation(len(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e44dc5a",
   "metadata": {},
   "source": [
    "Model Training and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93269148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['Text_Cleaned'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train = data['Sentiment'].values[idx[:int(train_pct*len(data))]]\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "\n",
    "X_test = data['Text_Cleaned'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test = data['Sentiment'].values[idx[int(train_pct*len(data)):]]\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385caeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4632,) (4632,)\n",
      "(1159,) (1159,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape )\n",
    "print(X_test.shape, y_test.shape )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bc5dcbe",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732d8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Bert NLP model tokenizer to encode tweets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7895f873",
   "metadata": {},
   "source": [
    "Encoding Tweets for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "309a3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "        \n",
    "    # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "        \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa9d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  53\n"
     ]
    }
   ],
   "source": [
    "# Use this to determine max length for encoding\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Cleaned'].values]\n",
    "MAX_LEN = max([len(sent) for sent in encoded])\n",
    "print('Max length: ', MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55105c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  53\n",
      "torch.Size([4632, 53]) torch.Size([4632, 53]) torch.Size([4632])\n",
      "torch.Size([1159, 53]) torch.Size([1159, 53]) torch.Size([1159])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode the tweets for Bert model\n",
    "def preprocessing_for_bert(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for line in data:\n",
    "        # encode the data. Return input encoding and attention mask\n",
    "        encoding = tokenizer.encode_plus(\n",
    "                text=line, # data to process\n",
    "                add_special_tokens=True, # adds special chars [CLS] and [SEP] to encoding \n",
    "                padding='max_length', # pad the tweets with 0s to fit max length\n",
    "                max_length = MAX_LEN, # assign max length\n",
    "                truncation=True, # truncate tweets longer than max length\n",
    "                return_tensors=\"pt\", # return tensor as pytorch tensor\n",
    "                return_attention_mask=True # return the attention mask\n",
    "                )\n",
    "\n",
    "        # add the encodings to the list\n",
    "        input_ids.append(encoding.get('input_ids'))\n",
    "        attention_masks.append(encoding.get('attention_mask'))\n",
    "    \n",
    "    # return the lists as tensors\n",
    "    input_ids = torch.concat(input_ids)\n",
    "    attention_masks = torch.concat(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "# Use this to determine max length for encoding\n",
    "encoded = [tokenizer.encode(sent, add_special_tokens=True) for sent in data['Text_Cleaned'].values]\n",
    "MAX_LEN = max([len(sent) for sent in encoded])\n",
    "print('Max length: ', MAX_LEN)\n",
    "\n",
    "\n",
    "# Encode the train and test data for Bert\n",
    "X_train_inputs, X_train_masks = preprocessing_for_bert(X_train)\n",
    "X_test_inputs, X_test_masks = preprocessing_for_bert(X_test)\n",
    "\n",
    "# Get the train and test labels\n",
    "y_train_labels = torch.tensor(y_train)\n",
    "y_test_labels = torch.tensor(y_test)\n",
    "\n",
    "print(X_train_inputs.shape, X_train_masks.shape, y_train_labels.shape)\n",
    "print(X_test_inputs.shape, X_test_masks.shape, y_test_labels.shape)\n",
    "\n",
    "# Set batch size to 16. recommended 16 or 32 depending on GPU size \n",
    "batch_size = 16\n",
    "\n",
    "# Randomize the train data and define dataloader for model training \n",
    "train_data = TensorDataset(X_train_inputs, X_train_masks, y_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Randomize the test data and define dataloader for model testing\n",
    "test_data = TensorDataset(X_test_inputs, X_test_masks, y_test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ee2ed2a",
   "metadata": {},
   "source": [
    "Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6e3eca-556a-4ee7-b48e-ea5b9f1bc998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  |  Train Loss: 0.54333  |  Test Loss: 0.44172  |  Test Accuracy: 77.89\n",
      "Epoch: 2  |  Train Loss: 0.30591  |  Test Loss: 0.43419  |  Test Accuracy: 83.11\n",
      "Epoch: 3  |  Train Loss: 0.14389  |  Test Loss: 0.67964  |  Test Accuracy: 82.62\n",
      "Epoch: 4  |  Train Loss: 0.06126  |  Test Loss: 0.82994  |  Test Accuracy: 83.19\n"
     ]
    }
   ],
   "source": [
    "# Define the Bert NLP Classifier\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # Define the neurons for the final layer\n",
    "        input_layer = 768\n",
    "        hidden_layer = 50\n",
    "        output_layer = 2\n",
    "\n",
    "        # Use the pretrained Bert model for first section of NN\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Define a final layer to attach to the Bert model for custom classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_layer, hidden_layer), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_layer, output_layer))\n",
    "\n",
    "        # Freeze the model from updating\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    # Return classification from Bert model \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask)\n",
    "        h_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(h_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# Set random seed for repeatability\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Check if GPU is available and assign device \n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Initialize Bert Classifier\n",
    "model = BertClassifier(freeze=False)\n",
    "\n",
    "# Send model to device (GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Define model hyperparameters\n",
    "epochs = 4\n",
    "steps = len(train_dataloader) * epochs\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "# Define scheduler for training the optimizer \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n",
    "\n",
    "# Define cross entropy loss function \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# For the number of epochs\n",
    "for e in range(epochs):\n",
    "    # Assign model to train\n",
    "    model.train()\n",
    "\n",
    "    # Intialize loss to zero\n",
    "    train_loss = 0\n",
    "    \n",
    "    # For each batch\n",
    "    for batch in train_dataloader:\n",
    "        # Get batch inputs, masks and labels \n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Reset the model gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get classification of encoded values\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "        \n",
    "        # Calculate loss based on predictions and known values\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        \n",
    "        # Add loss to the running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update the model weights based on the loss \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over batch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Assign the model to evaluate    \n",
    "    model.eval()\n",
    "\n",
    "    # Initialize losses\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # For each batch\n",
    "    for batch in test_dataloader:\n",
    "        # Get encoding inputs, masks and labels\n",
    "        batch_inputs, batch_masks, batch_labels = batch\n",
    "        \n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Predict the input values without updating the model \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Convert predictions to 0 and 1\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate accuracy of model on test data \n",
    "        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "        test_acc += accuracy\n",
    "\n",
    "    # Calculate average loss and accuracy per each batch\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    # Print epoch information \n",
    "    print('Epoch: %d  |  Train Loss: %1.5f  |  Test Loss: %1.5f  |  Test Accuracy: %1.2f'%(e+1, train_loss, test_loss, test_acc))\n",
    "    \n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'stock_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dcfe2b-967c-4969-a086-fb326815bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aal - completed\n",
      "aapl - completed\n",
      "adbe - completed\n",
      "adp - completed\n",
      "adsk - completed\n",
      "akam - completed\n",
      "alxn - completed\n",
      "amat - completed\n",
      "amgn - completed\n",
      "amzn - completed\n",
      "atvi - completed\n",
      "avgo - completed\n",
      "bbby - completed\n",
      "bidu - completed\n",
      "bmrn - completed\n",
      "ca - completed\n",
      "celg - completed\n",
      "cern - completed\n",
      "chkp - completed\n",
      "chtr - completed\n",
      "cmcsa - completed\n",
      "cost - completed\n",
      "csco - completed\n",
      "csx - completed\n",
      "ctrp - completed\n",
      "ctsh - completed\n",
      "disca - completed\n",
      "disck - completed\n",
      "dish - completed\n",
      "dltr - completed\n",
      "ea - completed\n",
      "ebay - completed\n",
      "endp - completed\n",
      "esrx - completed\n",
      "expe - completed\n",
      "fast - completed\n",
      "fb - completed\n",
      "fisv - completed\n",
      "foxa - completed\n",
      "fox - completed\n",
      "gild - completed\n",
      "googl - completed\n",
      "goog - completed\n",
      "hsic - completed\n",
      "ilmn - completed\n",
      "inct - completed\n",
      "incy - completed\n",
      "intu - completed\n",
      "isrg - completed\n",
      "jd - completed\n",
      "khc - completed\n",
      "lbtya - completed\n",
      "lbtyk - completed\n",
      "lltc - completed\n",
      "lmca - completed\n",
      "lmck - completed\n",
      "lrcx - completed\n",
      "lrcx - completed\n",
      "lvnta - completed\n",
      "mar - completed\n",
      "mat - completed\n",
      "mdlz - completed\n",
      "mnst - completed\n",
      "msft - completed\n",
      "mu - completed\n",
      "mxim - completed\n",
      "myl - completed\n",
      "nclh - completed\n",
      "nflx - completed\n",
      "ntap - completed\n",
      "ntes - completed\n",
      "nvda - completed\n",
      "nxpi - completed\n",
      "orly - completed\n",
      "payx - completed\n",
      "pcar - completed\n",
      "pcln - completed\n",
      "pypl - completed\n",
      "qcom - completed\n",
      "qvca - completed\n",
      "regn - completed\n",
      "rost - completed\n",
      "sbac - completed\n",
      "sbux - completed\n",
      "sndk - completed\n",
      "srcl - completed\n",
      "stx - completed\n",
      "swks - completed\n",
      "symc - completed\n",
      "tmus - completed\n",
      "trip - completed\n",
      "tsco - completed\n",
      "tsla - completed\n",
      "txn - completed\n",
      "ulta - completed\n",
      "viab - completed\n",
      "vod - completed\n",
      "vrsk - completed\n",
      "vrtx - completed\n",
      "wba - completed\n",
      "wdc - completed\n",
      "wfm - completed\n",
      "xlnx - completed\n",
      "yhoo - completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Get the list of stock data to convert\n",
    "files = os.listdir('data/')\n",
    "\n",
    "# for each stock files\n",
    "for x in range(len(files)):\n",
    "    # open the excel file on the Stream sheet\n",
    "    stock = pd.read_excel('data/'+files[x] + '/export_dashboard_' + files[x], sheet_name='Stream')\n",
    "\n",
    "    # Assign the ticker name as a column\n",
    "    stock['Ticker'] = files[x].split('_')[0]\n",
    "    \n",
    "    # Convert string date times to datetime\n",
    "    stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "    stock['Hour'] = stock['Hour'].apply(lambda t: pd.Timedelta(hours=int(t[:2]), minutes=int(t[3:])))\n",
    "    stock['Datetime'] = stock['Date'] + stock['Hour']\n",
    "\n",
    "    # Rename column that holds the tweets content\n",
    "    stock.rename(columns = {'Tweet content':'Text'}, inplace = True)\n",
    "\n",
    "    # Pre process the tweet content\n",
    "    stock = preprocess.Preprocess_Tweets(stock)\n",
    "\n",
    "    # Remove excess columns\n",
    "    stock = stock[['Tweet Id', 'Ticker', 'Datetime', 'Text', 'Text_Cleaned', 'Favs', 'RTs', 'Followers', 'Following', 'Is a RT']]\n",
    "    \n",
    "    # Fill NAs in Favs, RTs, Followers and Following with 0\n",
    "    stock = stock.fillna(0)\n",
    "\n",
    "    # Encode processed tweets for Bert NLP model\n",
    "    stock_inputs, stock_masks = preprocessing_for_bert(stock['Text_Cleaned'].values)\n",
    "\n",
    "    # Put stock data in PyTorch dataloader for processing \n",
    "    stock_data = TensorDataset(stock_inputs, stock_masks)\n",
    "    stock_sampler = RandomSampler(stock_data)\n",
    "    stock_dataloader = DataLoader(stock_data, sampler=stock_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Assign model to evaluate \n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # For each batch\n",
    "    for batch in stock_dataloader:\n",
    "        # Get encoded inputs and masks \n",
    "        batch_inputs, batch_masks = batch\n",
    "\n",
    "        # Send variables to device (GPU if available)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "\n",
    "        # Predict classes with Bert for given inputs \n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "        # Convert predictions to 0s and 1s\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        predictions.append(preds)\n",
    "\n",
    "    # Combine all batch predictions\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    \n",
    "    # Add predictions to stock dataframe\n",
    "    stock['Sentiment'] = predictions\n",
    "    \n",
    "    # save predictions as new csv\n",
    "    stock.to_csv('data/'+files[x] +'/stock_data_sentiment.csv', index=False)\n",
    "    \n",
    "    # Show stock names as they are completed \n",
    "    print(files[x].split('_')[0], '- completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea57530-2bac-4033-9f83-dcdc3207dd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755011135857461\n",
      "0.8802816901408451\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch_inputs, batch_masks, batch_labels = batch\n",
    "\n",
    "    batch_inputs = batch_inputs.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "    batch_labels = batch_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_inputs, batch_masks)\n",
    "\n",
    "\n",
    "    preds = torch.argmax(logits, dim=1).flatten()\n",
    "    predictions.append(preds)\n",
    "        \n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "\n",
    "negatives = np.where(y_test==0)[0]\n",
    "TNs = np.where( (y_test==0) & (y_test==predictions) )[0]\n",
    "print(len(TNs)/len(negatives))\n",
    "\n",
    "positives = np.where(y_test==1)[0]\n",
    "TPs = np.where( (y_test==1) & (y_test==predictions) )[0]\n",
    "print(len(TPs)/len(positives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
